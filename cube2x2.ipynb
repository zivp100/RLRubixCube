{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install 'tensorflow==1.15.0'\nimport tensorflow as tf\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nimport random\nimport json\nimport gym\nimport pandas as pd\nimport numpy as np\n\n\n\nimport tensorflow as tf\nfrom kaggle_environments import make, evaluate\nfrom gym import spaces\n\n!apt-get update\n!apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev\n!pip install \"stable-baselines[mpi]==2.9.0\"\n\nfrom stable_baselines import PPO2\nfrom stable_baselines.common.policies import MlpPolicy\n\n\n\n\n#===============================================================================\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cube data structure\n\ndef L(cube):\n    # L : 1 -> 3, 3 -> 7, 7 -> 5, 5 -> 1\n    temp = cube[1]\n    cube[1] = cube[5]\n    cube[5] = cube[7]\n    cube[7] = cube[3]\n    cube[3] = temp\n  \ndef R(cube):\n    # R : 2 -> 4, 4 -> 8, 8 -> 6, 6 -> 2\n    temp = cube[2]\n    cube[2] = cube[6]\n    cube[6] = cube[8]\n    cube[8] = cube[4]\n    cube[4] = temp\n\ndef D(cube):\n    # D : 3 -> 4, 4 -> 8, 8 -> 7, 7 -> 3\n    temp = cube[3]\n    cube[3] = cube[7]\n    cube[7] = cube[8]\n    cube[8] = cube[4]\n    cube[4] = temp\n\ndef U(cube):\n    # U: 1 -> 2, 2 -> 6, 6 -> 5, 5 -> 1\n    temp = cube[1]\n    cube[1] = cube[5]\n    cube[5] = cube[6]\n    cube[6] = cube[2]\n    cube[2] = temp\n    \ndef F(cube):\n    # F : 1 -> 2, 2 -> 4, 4 -> 3, 3 -> 1\n    temp = cube[1]\n    cube[1] = cube[3]\n    cube[3] = cube[4]\n    cube[4] = cube[2]\n    cube[2] = temp\n\ndef B(cube):\n    # B : 5 -> 6, 6 -> 8, 8 -> 7, 7 -> 5\n    temp = cube[5]\n    cube[5] = cube[7]\n    cube[7] = cube[8]\n    cube[8] = cube[6]\n    cube[6] = temp\n\n\n    \n# 6 cubes actions\n\ndef is_solved(input_cube):\n    solved_cube =[0, 1, 2, 3, 4, 5, 6, 7, 8]\n    if (input_cube == solved_cube):\n        return True\n    else:\n        return False\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CubeEnv(gym.Env):\n    \"\"\"A rubix cube environment for OpenAI gym\"\"\"\n    metadata = {'render.modes': ['human']}\n\n    def __init__(self):\n        super(CubeEnv, self).__init__()\n        #self.solved_cube = ['XXX','BYR', 'YGR', 'RGW', 'BRW', 'BOY', 'OGY', 'BOW', 'OWG']\n        self.solved_cube = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n        \n        self.action_space = spaces.Discrete(6) # 6 possible moves\n        #self.observation_space = spaces.Discrete(len(self.solved_cube)) # 8 pieces on the cube      \n        self.observation_space = spaces.Box(low=0, high=8, shape=(1,9), dtype=np.int8)\n      \n        self.solved = 0;\n        #  \n        \n       # self.reset()\n\n    def reset(self):\n        # Reset the state of the environment to an initial state\n        self.cube = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n        self.steps = 0\n                \n        # Reset - Start from solved cube and do 10-20 random actions\n        scramble_steps = random.randint(1,5)\n        \n        for i in range(scramble_steps):\n            direction = random.randint(1,6)\n           # print(self.cube, direction)\n            if (direction == 1):\n                R(self.cube)\n            if (direction == 2):\n                L(self.cube)\n            if (direction == 3):\n                B(self.cube)\n            if (direction == 4):\n                D(self.cube)\n            if (direction == 5):\n                F(self.cube)\n            if (direction == 6):\n                U(self.cube)\n        \n        self.start_cube = self.cube[:]        \n        self.obs = self.cube\n        return np.array([self.obs])\n    \n\n    def step(self, action):\n        # Execute one time step within the environment\n       \n        # Do \"action\" in the environment\n        if (action == 1):\n             R(self.cube)\n        if (action == 2):\n             L(self.cube)\n        if (action == 3):\n             B(self.cube)\n        if (action == 4):\n             D(self.cube)\n        if (action == 5):\n             F(self.cube)\n        if (action == 6):\n             U(self.cube)\n\n        # Increase steps\n        self.steps += 1\n\n        reward = 0\n        # Calculate reward\n        if (is_solved(self.cube)):\n            reward = 1000\n            print(\"Solved!! Steps=\", self.steps, \"Start=\", self.start_cube)\n            done = True\n            self.solved += 1\n        else:\n            if (self.steps > 200):\n                #Quits += 1\n                done = True\n            else:\n                reward = -1\n                done = False\n\n        obs = np.array([self.cube])\n\n        return obs, reward, done, {}\n\n\n#    def _on_training_end(self):\n#        \"\"\"\n#        This event is triggered before exiting the `learn()` method.\n#        \"\"\"\n#        print(\"Solved = \", self.solved)\n    \n\n    def render(self, mode='human', close=False):\n        # Render the environment to the screen\n        \n        print(f'Step: {self.steps}')\n        print(f'Cube: {self.cube}')\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from stable_baselines.common.vec_env import DummyVecEnv\ncube1 = CubeEnv()\ncube_env = DummyVecEnv([lambda: cube1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from stable_baselines.ppo2 import PPO2\nfrom stable_baselines.common.schedules import LinearSchedule\nfrom stable_baselines.common.policies import MlpPolicy\n# defining your network\n\npolicy_kwargs = dict(act_fun=tf.nn.relu, net_arch=[128, 128])\ncube_model = PPO2('MlpPolicy',  cube_env, verbose=0 #, \n                  # policy_kwargs=policy_kwargs, \n                  # learning_rate=LinearSchedule( 40000*5, initial_p=0.0005, final_p=0.00005).value, \n                  #tensorboard_log=\"./mlp\"\n                 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# auxiliary function to evaluate model\ndef evaluate(model, env, num_steps=1000):\n  \"\"\"\n  Evaluate a RL agent\n  :param model: (BaseRLModel object) the RL Agent\n  :param num_steps: (int) number of timesteps to evaluate it\n  :return: (float) Mean reward for the last 100 episodes\n  \"\"\"\n  episode_rewards = [0.0]\n  obs = env.reset()\n #! print(\"obs = \", obs)\n  \n  solved = 0;\n  for i in range(num_steps):\n      # _states are only useful when using LSTM policies\n      action, _states = model.predict(obs)\n      # print(\"action = \", action)\n      # here, action, rewards and dones are arrays\n      # because we are using vectorized env\n      obs, rewards, dones, info = env.step(action)\n      #! print(\"obs = \", obs, \"action =\", action, \"rewards = \", rewards, \"dones = \", dones)\n      \n      if rewards[0] == 1000:\n            solved += 1\n            \n      # Stats\n      episode_rewards[-1] += rewards[0]\n      if dones[0]:\n          #print(\"Solved!! obs = \", obs)\n          \n          obs = env.reset()\n          episode_rewards.append(0.0)\n  # Compute mean reward for the last 100 episodes\n  mean_100ep_reward = round(np.mean(episode_rewards[-100:]), 1)\n  print(\"Mean reward:\", mean_100ep_reward, \"Num of episodes:\", len(episode_rewards), \"Num times solved:\", solved)\n  \n  return mean_100ep_reward","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate(cube_model, cube_env, num_steps = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import time\n#start = time.time()\n#end = time.time()\n#print('training: ', end-start)\n\nevaluate(cube_model, cube_env, num_steps = 10000)\n\ncube_model.learn(500000)\n#print(\"Quits = \", Quits, \"Solves = \",Solves)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate(cube_model, cube_env, num_steps = 10000)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Bigger NN ?\nBetter definintion of observation space","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = YourEnv()\nobs = env.reset()\nn_steps = 10\nfor _ in range(n_steps):\n    # Random action\n    action = env.action_space.sample()\n    obs, reward, done, info = env.step(action)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}